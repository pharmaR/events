---
title: |
  R in Regulated Industries:
  Assessing Risk with {riskmetric}
author:
  - Doug Kelkhoff, on behalf of the R Validation Hub
  - Yilong Zhang, Marly Cormar, Eli Miller, Eric Milliman, Mark Padgham, Juliane
      Manitz
output:
  xaringan::moon_reader:
    css: ["riskmetric.css", "useR", "useR-fonts"]
    nature:
      ratio: 16:9
---

```{r, include=FALSE}
options(width = 42L)
library(dplyr)
library(riskmetric)
```

# Overview

1. Quick intro of unique challenges of using R in regulated industries
1. A brief overview of {riskmetric}'s design goals
1. A dive into {riskmetric} internals:
   building a foundation for an extendable package metadata collection platform

---

# R in Regulated Industries

- Long histories with licensed, proprietary tools (especially in statistical analysis)
- R is a forerunner language for new methods & as a glue language for data
  science
- Many organizations in regulated industries are looking for ways to leverage
  this awesome resource
- But first we need to ensure reproducibility of analytic software including:
  - proof of reliability of packages and implementations
  - robust, reproducible installation 
  - documenting measures of risk used to justify the use of the software

---

# The R Validation Hub

- Building tools, processes and recommendations for the use of R in regulated
  industries
- Predominantly focused on the Pharmaceutical industry, with representation in
  Finance and Agriculture
  - About 60 companies/groups on our mailing list
  - And about 10 actively contributing
- Aims to patch these gaps using openly developed tools such as {riskmetric}

> For more details, please check out [pharmaR.org](www.pharmaR.org)

---

# {riskmetric}

Aims to provide tooling to assess a R packages and make informed, reproducible
decisions about the choice of software for regulated decision making.
([github.com/pharmaR/riskmetric](www.github.com/pharmaR/riskmetric))

### Some use cases

- A statistician wants to know if a package is used broadly enough to justify
  using it for exploration (measuring community trust, adoption)
- An analyst wants to know that they can easily share work with a reviewer
  (measuring development stability)
- An R system administrator wants to ensure that installing a package won't
  break a managed environment (measuring reliability, robustness)
- Quality assurance wants to know that a package isn't malicious (measuring
  security)

---

# {riskmetric}

Aims to provide tooling to assess a R packages and make informed, reproducible
decisions about the choice of software for regulated decision making.
([github.com/pharmaR/riskmetric](www.github.com/pharmaR/riskmetric))

### Unique challenges

- Lots of users with unique needs, with varying levels of sensitivity 
- Aim to assess a package a different parts of its lifecycle
  - pre-install
  - installation into an environment
  - long-term stability/reproducibility

---

# {riskmetric}

Aims to provide tooling to assess a R packages and make informed, reproducible
decisions about the choice of software for regulated decision making.
([github.com/pharmaR/riskmetric](www.github.com/pharmaR/riskmetric))

### Our design objectives

- Accommodate multiple sources of package metadata
  - Pulling data from web is fine for some, others want to recompute locally
- While collecting package metadata, avoid having to manage information dependencies
  - eg Need to parse `DESCRIPTION` to get `URL` field and query for open repo issues
  - However, many assessments may need a parsed `DESCRIPTION`
- Minimize re-calculation of common data
  - Critical for _computationally intensive_ metrics (like `R CMD check`) or
    _rate-limited_ API queries
- Want to encourage contribution & extension with low bar to entry

---

# {riskmetric} Data Flow

![Core Workflow: pkg_ref objects transformed using assess function into 
pkg metrics, which are then scored resulting in a numeric summary score)](assets/core-workflow.svg)

---

# {riskmetric} `pkg_ref`

.pull-left[
- Core class of `{riskmetric}`
- S3 subclass hierarchy or "remote" ("cran", "bioconductor", "github", etc),
  "install" or "source" package references
- A wrapper around an `environment` with indexing methods to handle lazy
  evaluation of fields
]

.pull-right[
```{r}
pkg_ref("riskmetric")
```
]

---

# {riskmetric} `pkg_ref`

.pull-left[
Instantiate a stateful `pkg_ref` object
```{r}
pkg <- pkg_ref("riskmetric")
pkg
```
]

.pull-right[
Indexing prompts stateful data collection
```{r}
aliases <- pkg$help_aliases
pkg
```
]

---

# {riskmetric} `pkg_ref`

Accessing a field in the class uses an S3 generic to retrieve content, which
often themselves dispatches on `pkg_ref` class to extract content from respective
reference sources in a unified (preferably atomic) format.

This allows developers to introduce new metrics, using fields without having to
manage interdependence and execution order.

```{r}
pkg_ref_cache.example <- function(x, name, ...) {
  UseMethod("pkg_ref_cache.example")
}

pkg_ref_cache.example.pkg_install <- function(x, name, ...) {
  length(pkg$help_aliases)
}

pkg$example
```

---

# {riskmetric} `assess_*` & `score_*`

The rest of the process is information management and data processing:

- `assess_*` functions extract pertinent data from collected metadata
- `score_*` functions score risk on a numeric scale
- `summarize_scores` aggregates scores into a summary risk measure

```{r, eval = FALSE}
pkg_ref(c("riskmetric", "utils", "tools")) %>%
  pkg_assess() %>%
  pkg_score()
```

```{r, include = FALSE}
options(width = 80L)
```

```{r, echo = FALSE, width = 80L}
pkg_ref(c("riskmetric", "utils", "tools")) %>%
  pkg_assess() %>%
  pkg_score(error_handler = score_error_NA)
```

---

# Summary

- We use a lazy, stateful S3 classed environment to ease order-dependent
  evaluation cognitive burden & execution management
- We leverage S3 dispatch to make it easy to develop new metrics with minimal
  class system awareness
- We "double dispatch" on both field and subclass to provide a mechanism of
  providing alternative paths to deriving metric data, dependent on where data
  is being pulled from
- Overall, this has fostered good engagement with organizations and industry
  partners, many of which are relatively new to R

---

# Thanks!

### Special thanks to

- Yilong Zhang (Developer)
- Marly Cormar (Developer [Shiny app interface](www.github.com/pharmaR/risk_assessment))
- Eli Miller (Developer)
- Eric Milliman (Developer)
- Mark Padgham (Reviewer & keeps us aligned with ROpenSci's awesome work)
- Juliane Manitz (Developer)

### Happy to field questions

